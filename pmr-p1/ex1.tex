\documentclass[a4paper,fleqn,12pt]{article}
\input{preamble}

\title{\Huge{\textbf{Exam 1, IA}}}
\author{Mateus Marques}

\usepackage[shortlabels]{enumitem}

\begin{document}

\maketitle

TODO: estudar muito os slides \texttt{evalution.pdf}.

\section{Question 1}
$E =$ classifier $A$ makes an error. $C =$ correct decision.
$$
\begin{cases}
\; P(E) = 1/4 = x \\
\; P(C | \neg E) = 9/10 = y \\
\; P(C | E) = 3/10 = z \\
\end{cases}
$$
We want $P(\neg E| C)$:
$$
P(\neg E | C) = \frac{P(C | \neg E) P(\neg E)}{P(C)} =
\frac{P(C|\neg E) (1-P(E))}{P(C|E) P(E) + P(C|\neg E) P(\neg E)} =
\frac{y (1-x)}{z x + y (1-x)}
$$
$$
= \frac{9/10 \cdot 3/4}{3/10 \cdot 1/4 + 9/10 \cdot 3/4} =
\frac{9 \cdot 3}{3 + 9 \cdot 3} = 9/10.
$$

\section{Question 2}

$$
P(X = x | Y = y) = \frac{5(1-y) + yx}{5 \cdot (y + 10)}.
$$
We know $P(Y=1) = 2/3$.
$$
P(Y = 1 | X = x) P(X = x) = P(X = x | Y=1) P(Y = 1).
$$
$$
P(Y = -1 | X = x) P(X = x) = P(X = x | Y=-1) P(Y = -1).
$$
Therefore:
$$
f(x) = \frac{P(Y=1|X=x)}{P(Y=-1|X=x)} = \frac{P(X=x|Y=1)}{P(X=x|Y=-1)} \, \frac{P(Y=1)}{P(Y=-1)} =
\frac{x/55}{5/50} \cdot 2 = \frac{20}{55} \, x.
$$
The Bayes classifier generates $Y = 1$ if $f(x) \geq 1$ and $Y = -1$ if $f(x) < 1$.

We have $f(2) = 40/45$ and $f(8) = 160/55$.

\n

The answer is $(-1, 1)$.

\section{Question 3}

$\hat{f}(X) = 2X$, but $Y = X+2$ and $p_X(x) = x/2$. Also $p_Y(y) \dd{y} = p_X(x) \dd{x}$.
$$
E[(Y - \hat{Y})^2] = \int_{y(0)}^{y(2)} (Y-\hat{Y})^2 \, p_Y(y) \dd{y} =
\int_{0}^{2} (Y(x)-\hat{Y}(x))^2 \, p_X(x) \dd{x} =
$$
$$
= \int_{0}^{2} (x + 2 - 2x)^2 \, x / 2 \dd{x} = \frac{1}{2} \int_0^2 (x^3-4x^2+4x) \dd{x} =
\frac{1}{2} \, \qty(\frac{16}{4} - 4 \cdot \frac{8}{3} + 8) = 2/3.
$$

\section{Question 4}

When analyzing only label $1$ or not, we have the following confusion matrix:
$$
\begin{pmatrix}
35 & 5 \\
14 & 80
\end{pmatrix}
$$
The precision is $\frac{\text{true positive}}{\text{get the label}} = \frac{35}{35 + 5} = 7/8$.

\section{Question 5}

\begin{enumerate}[(a)]
\item No. We only have a estimative of something $\eps_{\text{1NN}} \leq 2 \eps_{\text{Bayes}}$.
\item No. There are not a combination of classifiers.
\item That is correct. Simple.
\item No. We only have a estimative of something $\eps_{\text{1NN}} \leq 2 \eps_{\text{Bayes}}$.
\item Obviously false, kNN has nothing to do with a brain.
\end{enumerate}

\section{Question 6}

I don't know about this one.

\section{Question 7}

$$
\text{error rate} = \frac{2000}{10000} = 0.2.
$$
The $\sigma$ is, where $\eps(i) = 1$ if the classifier missed and $\eps(i) = 0$ if it got right.
$$
\sigma^2 = \frac{1}{10000} \sum_{i=1}^{10000} (0.2 - \eps(i))^2 =
\frac{2000 \cdot 0.8^2 + 8000 \cdot 0.2^2}{10000} = 0.16 \implies \sigma = 0.04.
$$
The confidence interval with confidence 0.95 corresponds to $2\sigma$, therefore $[0.2 - 2\sigma, 0.2 + 2\sigma] = [0.192, 0.28]$.

\textbf{THIS IS WRONG}, olhar nos slides.



\end{document}
