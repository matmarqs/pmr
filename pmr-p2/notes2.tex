\documentclass[a4paper,fleqn,12pt]{article}
\input{preamble}

\title{\Huge{\textbf{P2 - Statistical Learning}}}
\author{Mateus Marques}

\begin{document}

\maketitle

\section{Neural Networks}

\subsection{Perceptron}

Perceptron são funções simples assim:
$$
Y = f\qty(w_0 + \sum_{i=1}^{n} w_i X_i),
$$
onde $f(X) = 1$ se $X \geq 0$, e $-1$ caso contrário.

Só que os perceptrons só conseguem resolver problemas linearmente separáveis (tipo SVM).
\begin{figure}[H]
\centering
\includegraphics[width=0.28\textwidth]{fig/perceptron.png}
\caption{Perceptron.}
\label{fig:perceptron}
\end{figure}

\subsection{Multi-Layer Perceptron (MLP)}

Com novas camadas temos mais liberdade:
$$
Y =  \beta_0 + \sum_{k=1}^{K} \beta_k \, g\qty(w_{k0} + \sum_{j=1}^{p} w_{kj} X_k),
$$
onde $p$ é o número de variáveis na camada de input e $K$ o número de variáveis na camada oculta.
\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{fig/mlp.png}
\caption{Multi-Layer Perceptron (MLP).}
\label{fig:mlp}
\end{figure}

Com camadas oculta, a função de ativação $g(\cdot)$ deve ser não-linear, do contrário teremos uma estrutura linear. Geralmente escolhemos $g(z)$ assim (\texttt{ReLU} é mais comum nos dias de hoje):
\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{fig/activation_function.png}
\caption{Função de ativação.}
\label{fig:units_activation_functions}
\end{figure}

Existe o chamado \textbf{Universal Approximation Result (1989)}: um Multi-Layer Perceptron (MLP) com uma única camada oculta consegue aproximar qualquer função contínua arbitrariamente bem.
\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{fig/example_mlp.png}
\caption{Exemplo de Multi-Layer Perceptron (MLP).}
\label{fig:example_mlp}
\end{figure}

No exemplo acima, os pesos são:
$$
\begin{cases}
\; \beta_0 = 0, \beta_1 = 1, \beta_2 = -1; \\
\; w_{10} = 0 , w_{11} = 1, w_{12} = 1; \\
\; w_{20} = -1, w_{21} = 1, w_{22} = 1.
\end{cases}
$$
Isso nos dá que
$$
Y = g\qty(X_1 + X_2) - g\qty(X_1 + X_2 - 1),
$$
onde $g(z) = 1$ se $z \geq 1/2$ e $g(z) = 0$ caso contrário.

Com muitos outputs, a esquematização é desse jeito:
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{fig/many_outputs.png}
\caption{Esquematização de muitos outputs.}
\label{fig:many_outputs}
\end{figure}

No caso de muitos outputs, por exemplo classificar uma imagem para saber qual dígito de 0-9 ela possui, é comum utilizarmos o \textit{one-hot coding}, usando assim 10 variáveis $Z_m$ com valores binários $m =$ 0 ou 1, ao invés de somente uma variável com 10 valores de 0-9.

Se todas os 10 outputs fossem separados e independentes, colocaríamos $f_m(X) = Z_m$ e seria isso mesmo, mas note que o \textit{one-hot coding} não é independente. Assim, interpretamos então $f_m(X) = \mathbb{P}(Y = m \mid X)$ e utilizamos a função especial \textit{softmax activation function}
$$
f_m(X) = \P(Y = m \mid X) = \frac{e^{Z_m}}{\sum_{\ell = 0}^9 e^{Z_\ell}}.
$$

Já que os outputs são qualitativos, para treinar esta rede tentamos estimar os coeficientes que minimizam a log-likelihood multinomial negativa
$$
- \sum_{i=1}^{n} \sum_{m=0}^{9} y_{im} \log(f_m(x_i)),
$$
também conhecida como \textit{cross-entropy}. Isso é uma generalização do critério de regressão logística para duas classes. Se os outputs fossem quantitativos, nós minimizaríamos o erro quadrático, como veremos na próxima seção.

\subsection{Treinando uma MLP}

Primeiro, escolher o número de camadas, número de variáveis em cada camada e as funções de ativação. Então, minimizamos o erro quadrático:
$$
R_\theta = \frac{1}{2} \sum_{j=1}^{N} (y_j - f_\theta(x_j))^2.
$$
O algoritmo que geralmente é usado em redes neurais para minimizar o erro quadrático é o \textit{gradiente descendente}. A cada iteração $t$, calculamos
$$
\grad{R_\theta^t} = \eval{\pdv{R_\theta}{\theta}}_{\theta^t}
$$
e tomamos $w^{t+1} = w^t - \rho \grad{R_\theta^t}$, onde $\rho$ é a \textit{taxa de aprendizado} (o número pequeno do gradiente descendente, nada demais).

Claramente, para calcularmos as derivadas de $R_\theta^t$, aplicamos a \textit{regra da cadeia} sem muito mistério.

Os algoritmos do estado da arte utilizam o \textit{gradiente descendente estocástico} para calcular as derivadas a partir de amostras dos dados selecionadas aleatoriamente.

Também é comum regularizar a minimização com ridge ou lasso:
$$
R(\theta; \lambda) = - \sum_{i=1}^{n} \sum_{m=0}^{9} y_{im} \log(f_m(x_i)) +
\lambda \sum_j \theta_j^2. \quad \text{(ridge)}
$$

\subsection{Dropout}

A ideia é do método de regularização \textit{dropout} é simplesmente remover aleatoriamente alguma das unidades (variáveis da camada de input) com probabilidade $p$ e reescalar todos as outras unidades por $\frac{1}{1-p}$ para compensar. É mais ou menos uma forma de fazer \textit{feature selection}.

\subsection{CNN e RNN}

Convolutional Neural Networks (CNN) são redes neurais (utilizadas para tratamento de imagens), só seus pesos $w_{kj}$ são mais estruturados. Interpretamos eles como sendo filtros convolucionais.
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{fig/cnn_architecture.png}
\caption{Arquitetura de uma CNN.}
\label{fig:cnn_architecture}
\end{figure}

Recurrent Neural Networks (RNN) são redes neurais utilizadas para tratamento de dados sequenciais (por exemplo séries temporais).
\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{fig/rnn_architecture.png}
\caption{Arquitetura de uma RNN.}
\label{fig:rnn_architecture}
\end{figure}

\subsection{Lembrete}

\begin{itemize}
\item Classificação: utilizamos \textit{softmax} e a \textit{cross-entropy}.
\item Regressão: utilizamos o erro quadrático.
\end{itemize}

\section{Árvores}

A ideia de métodos baseados em árvores é literalmente fazer um monte de \texttt{if} e \texttt{else} para ir classificando as coisas.
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{fig/tree_method.png}
\caption{Método baseado em árvores.}
\label{fig:tree_method}
\end{figure}
É fácil de entender e desenhar, é flexível em capturar fronteiras e consegue lidar com features contínuas ou categóricas.

Para acharmos bons \textit{splits}, para \textit{árvores de regressão} existe a ideia básica (escolha o melhor split para cada passo de forma \textit{greedy}) e para \textit{árvores de classificação}, existe o algoritmo \texttt{C4.5}, que é baseado em entropia.

\subsection{Árvores de classificação}

Utilizamos o algoritmo \texttt{C4.5} para classificação.

A entropia de uma conjunto de observações $D$ é
$$
I(D) = - \sum_{k=1}^{K} p_k \log(p_k),
$$
onde $p_k = \abs{D_k} / \abs{D}$ e $D_k$ é o subconjunto de observações tal que $Y = k$.

O ganho de informação de uma partição de $D$ em datasets $D^1, \ldots, D^m$ é
$$
G(D^1, \ldots, D^m) = I(D) - \sum_{j=1}^{m} \frac{\abs{D^j}}{\abs{D}} \, I(D^j).
$$

Assim, o \textit{gain ratio} é $\frac{G(D^1, \ldots, D^m)}{I(D)}$.

\n

O algoritmo é
\begin{enumerate}
\item Input: dataset de treino.
\item Se o dataset está vazio, STOP. Se todos os labels no dataset forem idênticos, retornar uma \textit{leaf} com esse label.
\item Para cada split possível, computar a \textit{gain ratio} da partição resultante do dataset de treino.
\item \textit{Greedy step}: selecionar o split com maior \textit{gain ratio}.
\item O split corresponde a um nodo. Cada element da partição do dataset é uma aresta ao nodo possível. Passo recursivo: chamar o algoritmo para cada tal elemento.
\end{enumerate}

Suponha que tenhamos uma região contendo algumas observações coletadas no dataset $D$. O índice Gini dessa região é
$$
G = \sum_{k=1}^{K} \frac{\abs{D_k}}{\abs{D}} \qty(1 - \frac{\abs{D_k}}{\abs{D}}) = 1 - \sum_{k=1}^{K} \qty(\frac{\abs{D_k}}{\abs{D}})^2.
$$
Ele não é nada mais do que expandir o log em primeira ordem em Taylor (entropia linear) e é uma medida de pureza (assim como na Mecânica Quântica, para a matriz densidade).

Podemos avaliar um split com a entropia ou com Gini index (que é uma aproximação da entropia).

\subsection{Árvores de regresão}

No caso de $Y$ ser uma variável contínua, a ideia é a mesma: fazer splits de forma \textit{greedy}, só que dessa vez minimizamos o RSS
$$
\sum_{j=1}^{J} \sum_{i \in R_j} (y_i - \hat{y}_{R_j})^2.
$$

Em cada passo, para cada $j$ e $s$ nós definimos os pares
$$
R_1(j, s) = \qty{X \mid X_j < s} \e R_2(j, s) = \qty{X \mid X_j \geq s},
$$
e procuramos os valores de $j$ e $s$ que minimizam a expressão
$$
\sum_{i : \, x_i \in R_1(j, s)} (y_i - \hat{y}_{R_1})^2 +
\sum_{i : \, x_i \in R_2(j, s)} (y_i - \hat{y}_{R_2})^2.
$$
No próximo passo, ao invés de splitar o espaço inteiro, nós splitamos uma das duas regiões identificadas previamente. O processo continua até um critério de parada ser satisfeito.

O processo descrito acima pode ter bons resultados no dataset de treino, mas é bem provável que aconteça um overfitting. Isso é porque a árvore resultante pode acabar sendo muito complexa.

Uma boa estratégia é montar uma árvore grande $T_0$ e depois cortá-la para obter uma sub-árvore menor.

Podemos selecionar um pequeno subconjunto de sub-árvores através do \textit{cost complexity pruning}. Nós consideramos uma sequência de árvores indexadas por um parâmetro $\alpha \geq 0$. Para cada valor de $\alpha$ corresponde uma sub-árvore $T \subset T_0$ tal que
$$
\sum_{m=1}^{\abs{T}} \sum_{i: \; x_i \in R_m} (y_i - \hat{y}_{R_m})^2
+ \alpha \abs{T}
$$
é o mínimo possível. Aqui, $\abs{T}$ indica o número de \textit{leaves} da árvore $T$. O parâmetro $\alpha$ controla o trade-off entre a complexidade da árvore e o seu fit ao dataset de treino. A ideia então é selecionar vários valores de $\alpha$ e aplicar validação cruzada para tuná-lo para o melhor possível que minimizar o erro médio.

\subsection{Vantagens e desvantagens}

\textbf{Vantagens:}
\begin{itemize}
\item Árvores são fáceis de entender.
\item Podem ser traduzidas em ``regras''.
\item São mais flexíveis que regressão logística (basicamente linear).
\item Conseguem lidar com variáveis contínuas ou categóricas.
\end{itemize}

\textbf{Desvantagens:}
\begin{itemize}
\item Não têm muita acurácia.
\item Não são robustas; muito sensíveis ao dataset de treino (variância é grande).
\end{itemize}

\subsection{Bagging trees}

Uma única árvore é boazinha. Mas por que não aprender um conjunto de árvores e fazer a média dos resultados? Essa ideia leva a \textit{bagging trees} e \textit{random forests}.

Bagging é o seguinte. Pegamos o dataset de treino e produzimos $B$ amostras dele próprio através de \textit{bootstrap}. Então tomamos a média para regressão e a maioria por votos para classificação. Isso é \textit{bagging}, e funciona para qualquer tipo de classificador. Geralmente é aplicado para \textit{weak classifiers}, que sozinhos não são muito bons mas com \textit{bagging} o resultado é melhorado. Esse é o caso para árvores de classificação ou regressão.

Para estimar o erro utilizando \textit{bagging}, não é necessário fazer validação cruzada. Basta que, para cada observação, nós produzimos a média de todas as árvores que foram aprendidas sem essa observação no conjunto de treino. Assim computamos o erro médio para essa observação. Isso é uma boa estimativa do erro, e quando $B \to \infty$ este erro tende ao erro do leave-one-out cross validation.

\subsection{Random forests}

O método de \textit{random forest} é dentro do contexto de \textit{bagging}. Só que, todavez ao fazer um split, nos restringimos a um subconjunto de $m$ features (das $p$ features/predictors $X_1, \ldots, X_p$) escolhido aleatoriamente para realizar o splitting. Normalmente toma-se $m \approx \sqrt{p}$.

Pela influência aleatória dos splis, as árvores tornam-se mais descorrelacionadas com as outras. Pela diminuição de correlação, a variância é reduzida.

Note que se tomássemos $m = p$ isso se reduziria ao caso normal de \textit{bagging}, já que qualquer split possível seria considerado.

Geralmente \textit{random forests} apresentam uma pequena melhoria com relação ao \textit{bagging} normal.

\subsection{Boosting}

\textit{Boosting} é um método geral para melhorar os resultados de um conjunto de \textit{weak classifiers} $g_k(X)$ (eles são fracos no sentido de que podem ser um pouco melhor do que chutes aleatórios, mas podem em tese ter boa acurácia se disponível).

A ideia de \textit{boosting} é combinar eles de maneira a produzir um classificador mais foda. Enquanto $g_1$ é aprendido no training dataset original, cada $g_k$ é aprendido numa versão ponderada do dataset original. Em outras palavras, sendo a variável de classe $Y \in \qty{-1,1}$, o \textit{boosted classifier} é
$$
g(x) = \text{sign}\qty(\sum_{k=1}^{M} \alpha_k g_k(x)),
$$
para alguns $\alpha_k$.

Ideia simples:
\begin{itemize}
\item Aprender $g_1$.
\item Aprender $g_2$, enfatizando que deve fazer a coisa certa quando $g_1$ cometer um erro.
\item Aprender $g_3$, enfatizando que deve fazer a coisa certa quando $g_1$ ou $g_2$ cometerem um erro. And so on...
\end{itemize}

O algoritmo mais famoso é o \texttt{AdaBoost}:
\begin{enumerate}
\item Inicializar $w_j = 1/N$, para $j \in \qty{1,\ldots,N}$.
\item Para $k$ de $1$ a $M$:
\begin{enumerate}
\item Aprender $g_k(X)$ no training dataset usando os pesos $w_j$.
\item Calcular
$$
\beta_k = \frac{\sum_{j=1}^{N} w_j \, I_{y_j \neq g_k(x^j)}}{\sum_{j=1}^{N} w_j}.
$$
\item Calcular $\alpha_k = \ln(\frac{1-\beta_k}{\beta_k})$.
\item Multiplicar cada peso $w_j$ por $\exp(\alpha_k I_{y_j \neq g_k(x^j)})$.
\end{enumerate}
\item Output $g(x) = \text{sign}\qty{\sum_{k=1}^{M} \alpha_k g_k(x)}$.
\end{enumerate}

Um esquema popular é utilizar \textit{boosting} com \textit{stumps} (troncos: árvores com um único split). Boosted classifiers com um grande número de stumps funciona muito bem.

O outro algoritmo de boosting é parecido com um gradiente descendente.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{fig/boosting_algo.png}
\caption{Algoritmo tipo gradiente de boosting.}
\label{fig:boosting_algo}
\end{figure}

\section{Aprendizado não-supervisionado}

Como não temos acesso aos rótulos $Y$ para checar nossos resultados, a ideia consiste em fazer uma análise exploratória para entender os dados melhor. Nisso entram técnicas de data visualization, agrupamento e estatística. Representation learning é justamente o processo de tentar representar os dados da melhor forma possível para aprender seus padrões.

\subsection{PCA}

O \textit{Principal Component Analysis} (PCA) é basicamente uma mudança de coordenadas para conseguir explorar as combinações de features que mais importam no dataset.

Assumindo que $X_1, \ldots, X_n$ sejam normalizados e com média zero, definimos cada componente principal $Z_i$ como a combinação linear das features original que tenham as maiores variâncias:
$$
Z_j = \phi_{1j} X_1 + \phi_{2j} X_2 + \cdots + \phi_{nj} X_n,
$$
onde $\sum_j \phi_{ij}^2 = 1$ e $\sigma_1^2 \geq \sigma_{2}^2 \geq \cdots \geq \sigma_p^2$, sendo cada $\sigma_j^2$ a variância de $Z_j$. Visualmente a ideia é tipo essa aqui
\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{fig/pca_variables.png}
\caption{Ideia de achar as componentes principais no PCA.}
\label{fig:pca_variables}
\end{figure}

Os coeficientes $\phi_{ij}$ são chamados de \textit{loadings}. A variância de $Z_1$ é
$$
\sigma_1^2 = \sum_{j=1}^{N} \qty( \sum_{i=1}^{n} \phi_{n1} x_{ij} )^2.
$$
Para acharmos os loadings, temos que maximizar
$$
\max_{\phi_{11}, \ldots, \phi_{n1}} \sum_{j=1}^{N} \qty( \sum_{i=1}^{n} \phi_{n1} x_{ij} )^2,
$$
com o vínculo $\sum_{i=1}^{n} \phi_{i1}^2 = 1$. É um problema quadrático que pode ser resolvido com good software. Após achar $Z_1$, achar $Z_2$, $Z_3$ até $Z_n$ para algum $n$ que seja bom.
\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{fig/example_3d.png}
\caption{Exemplo em três dimensões ($N = 3$) e duas componentes principais ($n=2$).}
\label{fig:example_3d}
\end{figure}
Se utilizarmos somente as componentes principais, conseguimos explicar a maior parte da variância dos dados. Com esperança, com as componentes principais temos features melhores.

A variância total é
$$
\frac{1}{N} \sum_{i=1}^{n} \sum_{j=1}^{N} x_{ij}^2.
$$
A variância é explicada pela componente principal $Z_k$ é
$$
\frac{1}{N} \sum_{j=1}^{N} z_{jk}^2.
$$

\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{fig/manifold_techniques.png}
\caption{Outras técnicas de manifolds.}
\label{fig:manifold_techniques}
\end{figure}

Quantas componentes principais usar? Pergunta difícil de responder. Mas uma ideia prática é checar a proporção das variâncias e parar quando elas forem baixas o suficiente.

O PCA é uma ferramenta para explorar os dados, mas também pode ser usada para produzir um conjunto mais compacto de features.

O algoritmo para resolver o PCA na prática é traduzido em termos de autovalores e autovetores, e diagonalização de matrizes simétricas.

Consideramos a matriz
$$
\vb{X} =
\begin{pmatrix}
\vb{x}_1 \\
\vb{x}_2 \\
\cdots \\
\vb{x}_N \\
\end{pmatrix}
$$
e então calculamos
$$
\frac{1}{N-1} \vb{X}^T \vb{X}.
$$
Cada elemento dessa matriz é a covariância
$$
\vb{A} = \frac{1}{N-1} \sum_{i=1}^{N} x_{ij} x_{kj}.
$$
As componentes principais então são dadas por
$$
\begin{pmatrix}
Z_1 \\
\vdots \\
Z_n \\
\end{pmatrix} =
\vb{U}^T
\begin{pmatrix}
X_1 \\
\vdots \\
X_n \\
\end{pmatrix},
$$
onde $\vb{U}$ é a matriz de autovetores $\vb{A}$. O melhor algoritmo para resolver isso \textit{singular value decomposition} (SVD).

Resumindo:
\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{fig/pca_summary.png}
\caption{Algoritmo para achar as componentes principais $Z_k$ do PCA.}
\label{fig:pca_summary}
\end{figure}

\subsection{Clustering}

A ideia é achar subgrupos do dataset, cada subgrupo é chamado de \textit{cluster}. É um pouco subjetivo e tenta realizar uma medida de similaridade. A ideia é minimizar similaridades intra-cluster e maximizar similaridades inter-clusters. Existem dois métodos importantes: K-means e hierarchical clustering.
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{fig/k-means_example.png}
\caption{Exemplo de clustering com $K-$means.}
\label{fig:k-means_example}
\end{figure}

Como dito anteriormente, devemos achar clusters $C_1, \ldots, C_K$ que maximizem
$$
\max_{C_1, \ldots, C_K} \sum_{k=1}^{K} s(C_k)
$$
para alguma medida $s$ de intra-cluster similarity. Uma medida razoável é
$$
s(C_k) = - \frac{1}{\abs{C_k}} \sum_{x',x''\in C_k} \sum_{i=1}^{n} (x_i'-x_i'')^2.
$$

\subsubsection{K-means}

O $K-$means não é um algoritmo estrito que maximiza a quantidade acima, mas é um método aproximado que dá uma solução razoável para esse problema.

O algoritmo do $K-$means é
\begin{enumerate}
\item Input: dataset, medida de distância, número $K$.
\item Selecionar a atribuição de $K$ clusters iniciais.
\begin{itemize}
\item Uma possibilidade: selecionar eles aleatoriamente.
\item Outra possibilidade: selecionar aleatoriamente $K$ pontos como centroides e atribuir pontos mais próximos aos centroides.
\end{itemize}
\item Iterar até um critério de parada for atingido.
\begin{itemize}
\item Calcular um centroide para cada cluster.
\item Agrupar cada ponto ao centroide mais próximo (utilizar a métrica de distância).
\end{itemize}
\end{enumerate}
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{fig/k-means_it.png}
\caption{Iterações de $K-$means.}
\label{fig:k-means_it}
\end{figure}

Quando parar?
\begin{itemize}
\item Depois de um número máximo de iterações.
\item Quando os centroides pararem de mudar (ou tiverem pouca mudança).
\item Quando a medida $\sum_{k=1}^{K} s(C_k)$ tiver pouca mudança.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{fig/k-means_strengths.png}
\caption{Vantagens do $K-$means.}
\label{fig:k-means_strengths}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{fig/k-means_weaknesses.png}
\caption{Fraquezas do $K-$means.}
\label{fig:k-means_weaknesses}
\end{figure}


\subsubsection{Hierarchical clustering}

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{fig/hierarchical_algo.png}
\caption{Algoritmo de hierarchical clustering.}
\label{fig:hierarchical_algo}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{fig/hierarchical_scheme.png}
\caption{Esquematização de hierarchical clustering.}
\label{fig:hierarchical_scheme}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{fig/hierarchical_distances.png}
\caption{Maneiras de calcular distâncias entre clusters.}
\label{fig:hierarchical_distances}
\end{figure}

\section{Decisões sequenciais e Aprendizado por reforço}

\subsection{Markov decision processes (MDPs)}

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{fig/mdps.png}
\caption{Descrição de MDPs.}
\label{fig:mdps}
\end{figure}

\subsection{Reinforcement Learning}

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{fig/reinforcement.png}
\caption{Descrição de Reinforcement Learning.}
\label{fig:reinforcement}
\end{figure}




\end{document}
